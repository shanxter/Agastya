ğŸ’¡ Agastya: LLM-Powered Assistant for HCPs (Healthcare Professionals)
------------------------------------------------------------

ğŸ§  OVERVIEW: How the LLM System Works

Agastya is a Python-based LLM system built to assist Healthcare Professionals by answering queries related to:

1. Real-time research/article queries
2. Conference info lookup
3. Panel support queries (honorarium, profile updates, etc.)

The entire system is orchestrated using LangGraph, which allows for controlled, node-based flow logic. 
The agent is enhanced with brief memory via Agent State, enabling short-term contextual reasoning within sessions.

All embeddings and retrievals are managed through ChromaDB, a local vector store solution, allowing full on-premise control over medical literature and structured knowledge.

------------------------------------------------------------

ğŸ” Query Resolution Flow (Simplified Logic)

User Input Query
    â†“
LangGraph: Classify Query Type Node
    â†“
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Panel      â”‚ Conference Info Lookup   â”‚ Research / Knowledge Lookup    â”‚
 â”‚ Support    â”‚                          â”‚                                â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†“                      â†“                                â†“
  SQL Query             API or Static File Lookup        Embed Query â†’ ChromaDB â†’ Retrieve Vectors
      â†“                      â†“                                â†“
                   Final LLM Answer Based on Contextual Info (RAG)
                                   â†“
                            Final User Response

------------------------------------------------------------

ğŸ—ï¸ DIRECTORY STRUCTURE

agastya-llm-assistant/
â”‚
â”œâ”€â”€ ingest_pipeline/               # Fetch, chunk, and embed content
â”‚   â”œâ”€â”€ fetch_content.py           # API calls to PubMed, arXiv, etc.
â”‚   â”œâ”€â”€ chunk_text.py              # Clean & chunk content
â”‚   â”œâ”€â”€ embed_and_store.py         # Embed and store in ChromaDB
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ chroma_store/              # Local ChromaDB instance
â”‚   â””â”€â”€ sql_connector.py           # Connects to internal SQL for panel support queries
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ langgraph_flow.py          # Node and edge definitions for LangGraph orchestration
â”‚   â”œâ”€â”€ memory.py                  # Manages brief session memory using Agent State
â”‚   â””â”€â”€ main_agent.py              # Entry point for query classification and routing
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ api_keys.yaml              # Secure keys for research APIs (e.g., arXiv, PubMed)
â”‚   â”œâ”€â”€ vectordb_config.yaml       # Chroma DB setup details
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_vector_pipeline.py    # Ensure vector creation and retrieval works as expected
â”‚
â””â”€â”€ run.py                         # Bootstraps the agent and starts interaction loop

------------------------------------------------------------

âœ… TECH STACK

- Python 3.10+
- LangGraph (orchestration)
- ChromaDB (local vector DB)
- OpenAI / Anthropic LLM APIs
- SQLAlchemy (panel query access)
- PyPDF, BeautifulSoup (for chunking and ingesting data)
- FastAPI (if deployed as a web app)

------------------------------------------------------------

ğŸ§  MEMORY & CONTEXT HANDLING
- Agent State stores lightweight memory: recent queries, classification decision, tool usage.
- Does not store PII or long-term sessions.
- Can be extended to support multi-turn conversations if needed.

------------------------------------------------------------

ğŸ“Œ NEXT STEPS (OPTIONAL)
- Add UI wrapper (Streamlit/FastAPI)
- Add usage logging
- Add eval pipeline (precision of panel answers, latency of research answers, etc.)
